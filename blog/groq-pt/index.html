<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/normalize.css">
<link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>PromptingTools.jl supports Groq</title>  
</head>
<body>
<div class="center-column-holder">
  <div class="center-column">
  <header>
  <span style="font-weight: bold;">cameron pfiffer</span>
  | <a href="/">about</a>
  ∘ <a href="/blog/">blog</a>
  ∘ <a href="/links/">links</a> <hr/>
</header>

<!-- Content appended here -->
<div class="franklin-content">
<h1 id="promptingtoolsjl_supports_groq"><a href="#promptingtoolsjl_supports_groq" class="header-anchor">PromptingTools.jl supports Groq</a></h1>
<p>PromptingTools.jl, one of my favorite Julia packages for generative AI workflows, now supports groq&#33;  For those who do not know, groq is <a href="https://wow.groq.com/groq-sets-new-large-language-model-performance-record-of-300-tokens-per-second-per-user-on-meta-ai-foundational-llm-llama-2-70b/">incredibly fast</a>. Of the cloud providers for LLM generation, groq is by far the fastest.</p>
<p>You&#39;ll need PromptingTools.jl version 0.22. The release notes are <a href="https://github.com/svilupp/PromptingTools.jl/releases/tag/v0.22.0">here</a>.</p>
<p>Here&#39;s a little demo of how to use this. To start, you&#39;ll need a groq API key, which you can find on the website &#40;<a href="https://console.groq.com/keys">this link</a> might work?&#41;.</p>
<p>Put your key in the environment variable <code>GROQ_API_KEY</code>. If you haven&#39;t done this at the system level, you can do it inside Julia like so:</p>
<pre><code class="language-julia">ENV&#91;&quot;GROQ_API_KEY&quot;&#93; &#61; &quot;your_key_here&quot;</code></pre>
<p>Great. Now we can use PromptingTools:</p>
<pre><code class="language-julia">using PromptingTools
using PromptingTools: GroqOpenAISchema


# Create the schema
schema &#61; GroqOpenAISchema&#40;&#41;
some_julia_code &#61; aigenerate&#40;
    schema,
    &quot;&quot;&quot;
    Give me some Julia code to calculate the n-th Fibonacci number.
    &quot;&quot;&quot;,
    model&#61;&quot;gllama370&quot;
&#41;

# Show the result
println&#40;some_julia_code.content&#41;</code></pre>
<p>which yielded &#40;for me&#41; the response</p>
<hr />
<p>Here is an example of Julia code to calculate the n-th Fibonacci number:</p>
<pre><code class="language-julia">function fibonacci&#40;n::Int&#41;
    if n &#61;&#61; 1
        return 0
    elseif n &#61;&#61; 2
        return 1
    else
        a, b &#61; 0, 1
        for i in 3:n
            a, b &#61; b, a &#43; b
        end
        return b
    end
end</code></pre>
<p>This function uses a simple iterative approach to calculate the n-th Fibonacci number. It takes an integer <code>n</code> as input and returns the corresponding Fibonacci number.</p>
<p>Here&#39;s an explanation of how the code works:</p>
<ul>
<li><p>The function takes an integer <code>n</code> as input and returns the n-th Fibonacci number.</p>
</li>
<li><p>The first two Fibonacci numbers are 0 and 1, so we handle these cases explicitly.</p>
</li>
<li><p>For <code>n &gt; 2</code>, we use a loop to calculate the n-th Fibonacci number. We initialize two variables <code>a</code> and <code>b</code> to 0 and 1, respectively, which correspond to the first two Fibonacci numbers.</p>
</li>
<li><p>In each iteration of the loop, we update <code>a</code> and <code>b</code> by swapping their values and adding the previous value of <code>a</code> to <code>b</code>. This is equivalent to calculating the next Fibonacci number as the sum of the previous two.</p>
</li>
<li><p>After <code>n-2</code> iterations, <code>b</code> will contain the n-th Fibonacci number, which we return as the result.</p>
</li>
</ul>
<p>You can test this function with a specific value of <code>n</code>, for example:</p>
<pre><code class="language-julia">julia&gt; fibonacci&#40;10&#41;
55</code></pre>
<hr />
<p>This isn&#39;t <em>quite</em> right. Calling this function with <code>fibonacci&#40;10&#41;</code> yields 34, not 55. This seems to be due to llama3 shifting the function up by one – <code>fibonacci&#40;0&#41;</code> should be 0, but here <code>fibonacci&#40;1&#41;</code> is 0.</p>
<p>But it&#39;s close enough for a prompt&#33;</p>
<p>You can also use string macros to make this a bit more concise:</p>
<pre><code class="language-julia"># Instead, you can also do string macros. You can do this by preceding
# the string with &#96;ai&#96; and following it with the model you want to use.
# In this case, we want to use groq&#39;s Llama3 70b &#40;gllama370&#41; model.
ai&quot;Give me some Julia code to calculate the n-th Fibonacci number.&quot;gllama370</code></pre>
<p>This is in case you&#39;re working from the REPL and don&#39;t want to type out the <code>aigenerate</code> function call.</p>
<p>You can use providers that are not groq as well. All providers available in PromptingTools.jl are available <a href="https://siml.earth/PromptingTools.jl/dev/coverage_of_model_providers">here</a>,  but the list is quite long. Providers include</p>
<ul>
<li><p>OpenAI</p>
</li>
<li><p>vLLM</p>
</li>
<li><p>Ollama</p>
</li>
<li><p>Mistral</p>
</li>
<li><p>Databricks</p>
</li>
<li><p>Fireworks AI</p>
</li>
<li><p>Together AI</p>
</li>
<li><p>Anthropic</p>
</li>
<li><p>Google Gemini</p>
</li>
</ul>
<p>Lastly, if you want to use other model aliases &#40;like <code>gllama370</code>&#41;, you can check them out inside <code>PromptingTools.MODEL_ALIASES</code>:</p>
<pre><code class="language-julia">julia&gt; PromptingTools.MODEL_ALIASES

Dict&#123;String, String&#125; with 38 entries:
  &quot;local&quot;         &#61;&gt; &quot;local-server&quot;
  &quot;gpt4v&quot;         &#61;&gt; &quot;gpt-4-vision-preview&quot;
  &quot;gpt3&quot;          &#61;&gt; &quot;gpt-3.5-turbo&quot;
  &quot;gpt4&quot;          &#61;&gt; &quot;gpt-4&quot;
  &quot;firefunction&quot;  &#61;&gt; &quot;accounts/fireworks/models/firefunction-v1&quot;
  &quot;tllama3&quot;       &#61;&gt; &quot;meta-llama/Llama-3-8b-chat-hf&quot;
  &quot;gpt4t&quot;         &#61;&gt; &quot;gpt-4-turbo&quot;
  &quot;mistral-tiny&quot;  &#61;&gt; &quot;mistral-tiny&quot;
  &quot;mistrall&quot;      &#61;&gt; &quot;mistral-large-latest&quot;
  &quot;emb3small&quot;     &#61;&gt; &quot;text-embedding-3-small&quot;
  &quot;starling&quot;      &#61;&gt; &quot;starling-lm&quot;
  &quot;tllama370&quot;     &#61;&gt; &quot;meta-llama/Llama-3-70b-chat-hf&quot;
  &quot;oh25&quot;          &#61;&gt; &quot;openhermes2.5-mistral&quot;
  &quot;mistral-large&quot; &#61;&gt; &quot;mistral-large-latest&quot;
  &quot;gemini&quot;        &#61;&gt; &quot;gemini-pro&quot;
  &quot;gl3&quot;           &#61;&gt; &quot;llama3-8b-8192&quot;
  &quot;gllama370&quot;     &#61;&gt; &quot;llama3-70b-8192&quot;
  &quot;mistralm&quot;      &#61;&gt; &quot;mistral-medium-latest&quot;
  &quot;tmixtral22&quot;    &#61;&gt; &quot;mistralai/Mixtral-8x22B-Instruct-v0.1&quot;
  &quot;ollama3&quot;       &#61;&gt; &quot;llama3:8b-instruct-q5_K_S&quot;
  ⋮               &#61;&gt; ⋮</code></pre>
<p>Anyways – thanks to <a href="https://siml.earth/">Jan</a> for more incredible work&#33;</p>
<p>– Cameron</p>
<div class="page-foot">
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->

</div> <!-- end of center-column -->
</div> <!-- end of center-column-holder -->
    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
