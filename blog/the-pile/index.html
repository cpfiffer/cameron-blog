<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
   <link rel="stylesheet" href="/libs/highlight/styles/github.min.css">
   
  <link rel="stylesheet" href="/css/normalize.css">
<link rel="stylesheet" href="/css/franklin.css">
<link rel="stylesheet" href="/css/hypertext.css">
<link rel="icon" href="/assets/favicon.png">

   <title>Getting the Pile</title>  
</head>
<body>
<div class="center-column-holder">
  <div class="center-column">
  <header>
  <span style="font-weight: bold;">cameron pfiffer</span>
  | <a href="/">about</a>
  ∘ <a href="/blog/">blog</a>
  ∘ <a href="/links/">links</a> <hr/>
</header>

<!-- Content appended here -->
<div class="franklin-content">
<p>I&#39;ve been interested in various NLP stuff lately, as one might imagine with all the ChatGPT stuff going on. Something I&#39;ve become interested in is methods for anlayzing large amounts of text. I&#39;ve been looking at the <a href="https://pile.eleuther.ai/">Pile</a> dataset, which is a commonly-used dataset in NLP. I believe ChatGPT has been trained on it, as have many other large <a href="https://en.wikipedia.org/wiki/Foundation_models">foundation models</a>. </p>
<p>I&#39;m trying to download it to tinker with <a href="https://arxiv.org/abs/1905.10347">discrete normalizing flows</a> for token prediction. It&#39;s a big dataset – about 825GB uncompressed. Being a hardo, I wrote my only little cloning script to pull in all the new data. It&#39;s not very efficient, but it works. I&#39;ll probably write a better one later. </p>
<p>If you want to use this code, make sure to change the <code>data_dir</code> variable to wherever you want to store the data.</p>
<pre><code class="language-julia">using HTTP
using ProgressMeter
import SHA
import Downloads

# Pile root directory
pile_root &#61; &quot;https://the-eye.eu/public/AI/pile/&quot;
data_dir &#61; &quot;/data/the-pile/mirror/&quot;

# Links path
links_path &#61; joinpath&#40;data_dir, &quot;links.txt&quot;&#41;

function update_progress&#40;meter, total, now&#41;
    meter.n &#61; total
    if now &#61;&#61; total
        # println&#40;&quot;Done&#33;&quot;&#41;
    else
        update&#33;&#40;meter, now&#41;
    end
end

&quot;&quot;&quot;
Extract the links from a url and return them as a vector. Remove any any links that include
&quot;..&quot; in the path.
&quot;&quot;&quot;
function extract_links&#40;url&#41;
    # Send simple get query to pile root directory
    response &#61; HTTP.get&#40;url&#41;
    body &#61; String&#40;response.body&#41;

    # Extract hrefs from html
    hrefs &#61; eachmatch&#40;r&quot;&#40;?&lt;&#61;href&#61;\&quot;&#41;&#91;^\&quot;&#93;&#43;&quot;, body&#41;
    links &#61; map&#40;x -&gt; url * x.match, hrefs&#41;
    filter&#33;&#40;x -&gt; basename&#40;dirname&#40;x&#41;&#41; &#33;&#61; &quot;..&quot;, links&#41;

    # # Write links to file
    # open&#40;links_path, &quot;w&quot;&#41; do f
    #     for link in links
    #         println&#40;f, link&#41;
    #     end
    # end

    # Find all the links that are directories
    dirs &#61; filter&#40;x -&gt; endswith&#40;x, &quot;/&quot;&#41;, links&#41;

    # Call extract_links on each directory and concatenate the results
    for dir in dirs
        links &#61; vcat&#40;links, extract_links&#40;dir&#41;&#41;
    end

    # Remove duplicates
    return unique&#40;links&#41;
end

if &#33;isfile&#40;links_path&#41;
    # Send simple get query to pile root directory
    links &#61; extract_links&#40;pile_root&#41;

    # Write links to file
    open&#40;links_path, &quot;w&quot;&#41; do f
        for link in links
            println&#40;f, link&#41;
        end
    end

    # Fink the link that contains SHA
    sha_link &#61; links&#91;findfirst&#40;x -&gt; occursin&#40;&quot;SHA&quot;, x&#41;, links&#41;&#93;

    # Download the SHA file if it doesn&#39;t exist
    ddir &#61; joinpath&#40;data_dir, basename&#40;sha_link&#41;&#41;
    &#33;isdir&#40;dirname&#40;ddir&#41;&#41; &amp;&amp; mkdir&#40;dirname&#40;ddir&#41;&#41;
    if &#33;isfile&#40;ddir&#41;
        download&#40;sha_link, ddir&#41;
    end
end

# Read the SHA file
sha &#61; open&#40;ddir&#41; do f
    read&#40;f, String&#41;
end

# Split the SHA file into lines
lines &#61; split&#40;sha, &quot;\n&quot;&#41;

# Split each line into SHA and file name
lines &#61; map&#40;x -&gt; split&#40;x, &quot; &quot;&#41;, lines&#41;

# Filter out empty lines
lines &#61; filter&#40;x -&gt; length&#40;x&#41; &gt; 0, &#91;filter&#40;x -&gt; length&#40;x&#41; &gt; 0, line&#41; for line in lines&#93;&#41;

# Separate into filename and sha
filenames &#61; &#91;joinpath&#40;line&#91;2&#93;&#41; for line in lines&#93;
shas &#61; &#91;line&#91;1&#93; for line in lines&#93;

# Create a dictionary of filenames and shas
sha_dict &#61; Dict&#40;zip&#40;filenames, shas&#41;&#41;

# Open links file
links &#61; open&#40;links_path&#41; do f
    readlines&#40;f&#41;
end

# Filter out links ending in /
filter&#33;&#40;x -&gt; &#33;endswith&#40;x, &quot;/&quot;&#41;, links&#41;

# For each link, check if it&#39;s been downloaded
for link in links
    # Get the filename
    file_relative &#61; replace&#40;link, pile_root &#61;&gt; &quot;&quot;&#41;

    # Check if the file exists
    file &#61; joinpath&#40;data_dir, file_relative&#41;

    # Determine whether to re-download the file
    download_file &#61; if isfile&#40;file&#41;
        # Check if the file is the correct size
        file_size &#61; filesize&#40;file&#41;

        if file_size &#61;&#61; 0 
            true
        else
            sha_local &#61; open&#40;file&#41; do f
                SHA.sha2_256&#40;f&#41;
            end

            if haskey&#40;sha_dict, &quot;./&quot; * file_relative&#41;
                sha_local &#33;&#61; sha_dict&#91;&quot;./&quot; * file_relative&#93;
            else
                true
            end
        end
    else
        true
    end

    # Download the file if necessary
    if download_file
        # Create the directory if it doesn&#39;t exist
        &#33;isdir&#40;dirname&#40;file&#41;&#41; &amp;&amp; mkdir&#40;dirname&#40;file&#41;&#41;

        # Make the meter
        p &#61; ProgressMeter.Progress&#40;1; desc&#61;file_relative, dt&#61;1&#41;
        update_fun&#40;total, now&#41; &#61; update_progress&#40;p, total, now&#41;

        # Download the file
        println&#40;&quot;Downloading &#36;file&quot;&#41;
        Downloads.download&#40;link, file, progress&#61;update_fun&#41;
    end
end</code></pre>
<div class="page-foot">
    Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
</div>
</div><!-- CONTENT ENDS HERE -->

</div> <!-- end of center-column -->
</div> <!-- end of center-column-holder -->
    
    
        <script src="/libs/highlight/highlight.min.js"></script>
<script>hljs.highlightAll();hljs.configure({tabReplace: '    '});</script>

    
  </body>
</html>
